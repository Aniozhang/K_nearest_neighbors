# K nearest neighbors(KNN)
Hello everyone, I learned a new  algorithm and that is K nearest neighbors algorithm. Let's dive into it.ğŸğŸŠ

1ï¸âƒ£ Mathematical Explanation

Step 1: Compute the Distance

To determine the nearest neighbors, we compute the distance between the new data point XnewXnewâ€‹ and every other point in the dataset.

The most common distance metric is Euclidean Distance:
d(A,B)=(x2âˆ’x1)2+(y2âˆ’y1)2
d(A,B)=(x2â€‹âˆ’x1â€‹)2+(y2â€‹âˆ’y1â€‹)2
â€‹

For higher dimensions, it generalizes to:
d(Xi,Xj)=âˆ‘m=1n(Xi,mâˆ’Xj,m)2
d(Xiâ€‹,Xjâ€‹)=m=1âˆ‘nâ€‹(Xi,mâ€‹âˆ’Xj,mâ€‹)2
â€‹

Other distance metrics include:

    Manhattan Distance: d(A,B)=âˆ£x2âˆ’x1âˆ£+âˆ£y2âˆ’y1âˆ£d(A,B)=âˆ£x2â€‹âˆ’x1â€‹âˆ£+âˆ£y2â€‹âˆ’y1â€‹âˆ£
    Cosine Similarity: d(A,B)=Aâ‹…Bâˆ¥Aâˆ¥âˆ¥Bâˆ¥d(A,B)=âˆ¥Aâˆ¥âˆ¥Bâˆ¥Aâ‹…Bâ€‹

Step 2: Select the K Nearest Neighbors

Once we compute the distances to all data points, we:

    Sort the data points by distance.
    Pick the closest K points.

Step 3: Majority Voting (Classification)

If we are doing classification, we check the labels of the nearest kk points and take the majority vote.
y^=argâ¡maxâ¡câˆ‘iâˆˆK1(yi=c)
y^â€‹=argcmaxâ€‹iâˆˆKâˆ‘â€‹1(yiâ€‹=c)

Where:

    1(yi=c)1(yiâ€‹=c) is an indicator function (1 if true, 0 otherwise).
    KK is the set of k-nearest neighbors.
    y^y^â€‹ is the predicted class.

Step 4: Averaging (Regression)

If we are doing regression, we take the average of the kk nearest values:
y^=1kâˆ‘iâˆˆKyi
y^â€‹=k1â€‹iâˆˆKâˆ‘â€‹yiâ€‹
